{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "import os\n",
    "import unittest\n",
    "forward = True\n",
    "iri = False\n",
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 4\n",
    "\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "path_to_json = '/Users/cuiyeshuai/Documents/UG modules/submission/openprov/'\n",
    "json_file = '/Users/cuiyeshuai/Documents/UG modules/submission/datasets/CM-Buildings/Building729.0.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFunctions(unittest.TestCase):\n",
    "    def test_node_and_edge_number(self):\n",
    "        f = open(json_file, 'r')\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        prov_encoding = json_to_encoding(data, iri = iri, forward = forward, qualified_names = qualified_names)\n",
    "        node_number = len(list(prov_encoding[0].keys()))\n",
    "        edge_number = 0\n",
    "        for start in prov_encoding[1]:\n",
    "            for end in prov_encoding[1][start]:\n",
    "                edge_number += 1\n",
    "        file = json.decoder.JSONDecoder().decode(data)\n",
    "        node_number_file = 0\n",
    "        edge_number_file = 0\n",
    "        for rec_type in PROV_NODE:\n",
    "            if file.get(rec_type) is not None:\n",
    "                for x in file.get(rec_type):\n",
    "                    node_number_file += 1\n",
    "        for rec_type in PROV_EDGE:\n",
    "            if file.get(rec_type) is not None:\n",
    "                for x in file.get(rec_type):\n",
    "                    edge_number_file += 1\n",
    "        self.assertEqual(node_number, node_number_file)\n",
    "        self.assertEqual(edge_number, edge_number_file)\n",
    "    \n",
    "    def test_node_label(self):\n",
    "        f = open(json_file, 'r')\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        prov_encoding = json_to_encoding(data, iri = iri, forward = forward, qualified_names = qualified_names)\n",
    "        self.assertEqual(prov_encoding[0][\"BuildingVerification463\"][0], \"activity\")\n",
    "        self.assertEqual(prov_encoding[0][\"BuildingVerification463\"][1], frozenset([\"collabmap:BuildingVerification\"]))\n",
    "        #self.assertEqual(prov_encoding[0][\"BuildingVerification463\"][1], frozenset([\"http://www.orchid.ac.uk/ontologies/collabmap.owl#BuildingVerification\"]))\n",
    "    \n",
    "    def test_equal_encoding(self):\n",
    "        f = open(json_file, 'r')\n",
    "        data = f.read()\n",
    "        f.close()\n",
    "        prov = ProvDocument()\n",
    "        prov = document_to_encoding(ProvDocument.deserialize(json_file), iri = iri, forward = forward)\n",
    "        prov1 = json_to_encoding(data, iri = iri, forward = forward, qualified_names = qualified_names)\n",
    "        self.assertEqual(prov, prov1)\n",
    "    \n",
    "    def test_equal_encoding_all(self):\n",
    "        for file_name in [file for file in os.listdir(path_to_json) if file.endswith('0.json')]:\n",
    "            with open(path_to_json + file_name) as json_file:              \n",
    "                data = json_file.read()\n",
    "                prov = document_to_encoding(ProvDocument.deserialize(content=data), iri = iri, forward = forward)\n",
    "                prov1 = json_to_encoding(data, iri = iri, forward = forward, qualified_names = qualified_names)\n",
    "                self.assertEqual(prov, prov1)\n",
    "    \n",
    "    def test_type_generation(self):\n",
    "        for file_name in [file for file in os.listdir(path_to_json) if file.endswith('0.json')]:\n",
    "            with open(path_to_json + file_name) as json_file:              \n",
    "                data = json_file.read()\n",
    "                prov = json_to_encoding(data, iri = iri, forward = True, qualified_names = qualified_names)\n",
    "                prov1 = json_to_encoding(data, iri = iri, forward = False, qualified_names = qualified_names)\n",
    "                prov_type = type_generate(prov, 5, True, True)\n",
    "                prov_type_R = type_generate_R(prov1, 5, True, True)\n",
    "                self.assertEqual(prov_type, prov_type_R)\n",
    "    def test_type_generation_mixed(self):\n",
    "        for file_name in [file for file in os.listdir(path_to_json) if file.endswith('0.json')]:\n",
    "            with open(path_to_json + file_name) as json_file:              \n",
    "                data = json_file.read()\n",
    "                prov = json_to_encoding(data, iri = iri, forward = True, qualified_names = qualified_names)\n",
    "                prov_type = type_generate(prov, 5, False, False)\n",
    "                prov_type_mixed = type_generate_mixed(prov, 5, False, False)\n",
    "                self.assertEqual(prov_type, prov_type_mixed)\n",
    "    def test_feature_vector(self):\n",
    "        json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "        conf = SparkConf().setAppName(\"spark\").setMaster(\"local[*,20]\").set(\"spark.driver.memory\", \"10g\")\n",
    "        sc = SparkContext(conf=conf)\n",
    "        sc.setLogLevel(\"ERROR\")\n",
    "        spark = SparkSession(sc)\n",
    "        file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "        # (file_name, Graphic_encoding_of_ProvDocument)\n",
    "        #encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "        encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "        # (file_name, prov_types of nodes)\n",
    "        if forward:\n",
    "            types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "            # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "        else:\n",
    "            types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "        # (file_name, prov_types occurence in the graph)\n",
    "        types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "        # All prov_types in this collection of graphs\n",
    "        all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "        # Number of distinct prov_types\n",
    "        types_count = len(all_types)\n",
    "        # index_map for prov_types, prov_type -> index\n",
    "        index_map = {all_types[i]: i for i in range(types_count)}\n",
    "        # index -> prov_type\n",
    "        reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "        # Contruct feature vectors for each graph\n",
    "        sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "        a = types_count_rdd.take(100)\n",
    "        b = sparse_matrix_rdd.take(100)\n",
    "        for x in range(100):\n",
    "            sum_a = 0\n",
    "            sum_b = 0\n",
    "            for y in a[x][1].keys():\n",
    "                self.assertEqual(a[x][1][y], b[x][1][index_map[y]])\n",
    "                sum_a += a[x][1][y]\n",
    "            for z in range(len(b[x][1])):\n",
    "                sum_b += b[x][1][z]\n",
    "            self.assertEqual(sum_a, sum_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unittest.main(argv=[''], exit=False,verbosity=2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
