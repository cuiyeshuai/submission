{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "# functions\n",
    "from functions import *\n",
    "import time\n",
    "# ML\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/06 15:36:42 WARN Utils: Your hostname, cuiyeshuaideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.31.122 instead (on interface en0)\n",
      "22/04/06 15:36:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/06 15:36:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/04/06 15:36:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"spark\").setMaster(\"local[*,20]\").set(\"spark.driver.memory\", \"10g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Valor\": 0.0,\n",
    "    \"Instinct\": 1.0,\n",
    "    \"Mystic\": 2.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giao():\n",
    "    file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "    \n",
    "    encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "    # (file_name, prov_types of nodes)\n",
    "    if forward:\n",
    "        types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "        # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    else:\n",
    "        types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # (file_name, prov_types occurence in the graph)\n",
    "    types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "    # All prov_types in this collection of graphs\n",
    "    all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "    # Number of distinct prov_types\n",
    "    types_count = len(all_types)\n",
    "    print(types_count)\n",
    "    # index_map for prov_types, prov_type -> index\n",
    "    index_map = {all_types[i]: i for i in range(types_count)}\n",
    "    # index -> prov_type\n",
    "    # Contruct feature vectors for each graph\n",
    "    sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "    feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "    df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scaler_model = scaler.fit(df_features)\n",
    "    df_features = scaler_model.transform(df_features)\n",
    "    # Change the labels\n",
    "    df_labels = spark.read.csv(label_csv, header=True)\n",
    "    df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "    l = list(label_map.values())\n",
    "    df_labels = df_labels.where(df_labels.label.isin(l))\n",
    "    df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "    # Join the features and labels\n",
    "    df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "    # Oversample the training data\n",
    "    labels = [float(x) for x in label_map.values()]\n",
    "    count = {}\n",
    "    for x in labels:\n",
    "        count[x] = df.filter(df['label'] == x).count()\n",
    "    maxValue = max(count.values())\n",
    "    print(maxValue)\n",
    "    ratio = {}\n",
    "    for x in labels:\n",
    "        ratio[x] = maxValue/count[x]\n",
    "    dataframes = []\n",
    "    for x in labels:\n",
    "        if(count[x] == maxValue):\n",
    "            dataframes.append(df.filter(df['label'] == x))\n",
    "        else:\n",
    "            dataframes.append(df.filter(df['label'] == x).sample(withReplacement=True, fraction=ratio[x]))\n",
    "    train = dataframes[0]\n",
    "    for dataframe in dataframes[1:]:\n",
    "        train = train.union(dataframe)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [4,5,6]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "    end = time.time()\n",
    "    result.append(max(cvModel.avgMetrics))\n",
    "    result.append(end-start)\n",
    "    print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "    print(end-start)\n",
    "\n",
    "    start = time.time()\n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [5,10]).addGrid(lr.regParam, [0.2,0.3,0.4]).addGrid(lr.elasticNetParam, [0.6,0.7,0.8]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "    result.append(max(cvModel.avgMetrics))\n",
    "    result.append(end-start)\n",
    "    print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/PG-T/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/PG-T/graphs.csv\"\n",
    "label_map = {\n",
    "    \"Valor\": \"0.0\",\n",
    "    \"Instinct\": \"1.0\",\n",
    "    \"Mystic\": \"2.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n",
      "[(0.8344805505691006, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4}), (0.8360934537949072, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}), (0.8337125014139548, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 5, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6}), (0.8406130879695641, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4}), (0.8406130879695641, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}), (0.8390001847437576, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 10, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6}), (0.8405946309743628, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4}), (0.8398291853756752, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}), (0.8398291853756752, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 20, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6}), (0.8422357393951323, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 30, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4}), (0.8422357393951323, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 30, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 5}), (0.8414420886014815, {Param(parent='RandomForestClassifier_24123b47ea89', name='numTrees', doc='Number of trees to train (>= 1).'): 30, Param(parent='RandomForestClassifier_24123b47ea89', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6})]\n",
      "52.67583894729614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/06 15:37:46 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/04/06 15:37:46 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/04/06 15:37:49 ERROR Instrumentation: java.lang.IllegalArgumentException\n",
      "\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:259)\n",
      "\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:177)\n",
      "\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:163)\n",
      "\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:284)\n",
      "\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n",
      "\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:358)\n",
      "\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n",
      "\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2477)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$1(RDD.scala:860)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:859)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1231)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n",
      "\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n",
      "\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n",
      "\tat breeze.optimize.OWLQN$$anon$1.calculate(OWLQN.scala:75)\n",
      "\tat breeze.optimize.OWLQN$$anon$1.calculate(OWLQN.scala:72)\n",
      "\tat breeze.optimize.BacktrackingLineSearch.iterations(BacktrackingLineSearch.scala:30)\n",
      "\tat breeze.optimize.ApproximateLineSearch.minimize(LineSearch.scala:26)\n",
      "\tat breeze.optimize.ApproximateLineSearch.minimize$(LineSearch.scala:26)\n",
      "\tat breeze.optimize.BacktrackingLineSearch.minimize(BacktrackingLineSearch.scala:11)\n",
      "\tat breeze.optimize.OWLQN.determineStepSize(OWLQN.scala:87)\n",
      "\tat breeze.optimize.OWLQN.determineStepSize(OWLQN.scala:17)\n",
      "\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:60)\n",
      "\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n",
      "\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:74)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1009)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:628)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n",
      "\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.GeneratedMethodAccessor197.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o35843.fit.\n: java.lang.IllegalArgumentException\n\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:259)\n\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:177)\n\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:163)\n\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:284)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:358)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2477)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$1(RDD.scala:860)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.OWLQN$$anon$1.calculate(OWLQN.scala:75)\n\tat breeze.optimize.OWLQN$$anon$1.calculate(OWLQN.scala:72)\n\tat breeze.optimize.BacktrackingLineSearch.iterations(BacktrackingLineSearch.scala:30)\n\tat breeze.optimize.ApproximateLineSearch.minimize(LineSearch.scala:26)\n\tat breeze.optimize.ApproximateLineSearch.minimize$(LineSearch.scala:26)\n\tat breeze.optimize.BacktrackingLineSearch.minimize(BacktrackingLineSearch.scala:11)\n\tat breeze.optimize.OWLQN.determineStepSize(OWLQN.scala:87)\n\tat breeze.optimize.OWLQN.determineStepSize(OWLQN.scala:17)\n\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:60)\n\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:74)\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1009)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:628)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.GeneratedMethodAccessor197.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/cuiyeshuai/Documents/UG modules/submission/evaluation PG.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000028?line=1'>2</a>\u001b[0m specific_types_node \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000028?line=2'>3</a>\u001b[0m level \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000028?line=3'>4</a>\u001b[0m giao()\n",
      "\u001b[1;32m/Users/cuiyeshuai/Documents/UG modules/submission/evaluation PG.ipynb Cell 3'\u001b[0m in \u001b[0;36mgiao\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=83'>84</a>\u001b[0m \u001b[39m# train the model and select the best model using \"metricName\"(hyperparameter tuning)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=84'>85</a>\u001b[0m crossval \u001b[39m=\u001b[39m CrossValidator(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=85'>86</a>\u001b[0m     estimator\u001b[39m=\u001b[39mpipeline,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=86'>87</a>\u001b[0m     estimatorParamMaps\u001b[39m=\u001b[39mparamGrid,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=87'>88</a>\u001b[0m     evaluator\u001b[39m=\u001b[39mMulticlassClassificationEvaluator(metricName\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=88'>89</a>\u001b[0m     numFolds\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=89'>90</a>\u001b[0m     collectSubModels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=90'>91</a>\u001b[0m cvModel \u001b[39m=\u001b[39m crossval\u001b[39m.\u001b[39;49mfit(train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=91'>92</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/cuiyeshuai/Documents/UG%20modules/submission/evaluation%20PG.ipynb#ch0000002?line=92'>93</a>\u001b[0m \u001b[39mprint\u001b[39m(end\u001b[39m-\u001b[39mstart)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py:689\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=683'>684</a>\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=685'>686</a>\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=686'>687</a>\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=687'>688</a>\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=688'>689</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=689'>690</a>\u001b[0m     metrics[j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (metric \u001b[39m/\u001b[39m nFolds)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=690'>691</a>\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py:868\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py?line=865'>866</a>\u001b[0m \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m    <a href='file:///Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py?line=866'>867</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m--> <a href='file:///Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py?line=867'>868</a>\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py?line=122'>123</a>\u001b[0m job, i, func, args, kwds \u001b[39m=\u001b[39m task\n\u001b[1;32m    <a href='file:///Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py?line=123'>124</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py?line=124'>125</a>\u001b[0m     result \u001b[39m=\u001b[39m (\u001b[39mTrue\u001b[39;00m, func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    <a href='file:///Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py?line=125'>126</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py?line=126'>127</a>\u001b[0m     \u001b[39mif\u001b[39;00m wrap_exception \u001b[39mand\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py:689\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=683'>684</a>\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=685'>686</a>\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=686'>687</a>\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=687'>688</a>\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=688'>689</a>\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=689'>690</a>\u001b[0m     metrics[j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (metric \u001b[39m/\u001b[39m nFolds)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=690'>691</a>\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/util.py:326\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/util.py?line=322'>323</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/util.py?line=323'>324</a>\u001b[0m     \u001b[39m# Set local properties in child thread.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/util.py?line=324'>325</a>\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/util.py?line=325'>326</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/util.py?line=326'>327</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/util.py?line=327'>328</a>\u001b[0m     InheritableThread\u001b[39m.\u001b[39m_clean_py4j_conn_for_current_thread()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py:69\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=67'>68</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingleTask\u001b[39m():\n\u001b[0;32m---> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=68'>69</a>\u001b[0m     index, model \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(modelIter)\n\u001b[1;32m     <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=69'>70</a>\u001b[0m     \u001b[39m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=70'>71</a>\u001b[0m     \u001b[39m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=71'>72</a>\u001b[0m     \u001b[39m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=72'>73</a>\u001b[0m     \u001b[39m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/tuning.py?line=73'>74</a>\u001b[0m     metric \u001b[39m=\u001b[39m eva\u001b[39m.\u001b[39mevaluate(model\u001b[39m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py:69\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=66'>67</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo models remaining.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=67'>68</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcounter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=68'>69</a>\u001b[0m \u001b[39mreturn\u001b[39;00m index, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfitSingleModel(index)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py:126\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=124'>125</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfitSingleModel\u001b[39m(index):\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=125'>126</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m estimator\u001b[39m.\u001b[39;49mfit(dataset, paramMaps[index])\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py:159\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=156'>157</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(params, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=157'>158</a>\u001b[0m     \u001b[39mif\u001b[39;00m params:\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy(params)\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/pipeline.py?line=111'>112</a>\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/pipeline.py?line=112'>113</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/pipeline.py?line=113'>114</a>\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/pipeline.py?line=114'>115</a>\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/pipeline.py?line=115'>116</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=333'>334</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=334'>335</a>\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=335'>336</a>\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=336'>337</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=317'>318</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=318'>319</a>\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=319'>320</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=328'>329</a>\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=329'>330</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=330'>331</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/ml/wrapper.py?line=331'>332</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1305'>1306</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1308'>1309</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1309'>1310</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1311'>1312</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py?line=1312'>1313</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///Users/cuiyeshuai/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o35843.fit.\n: java.lang.IllegalArgumentException\n\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:259)\n\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:177)\n\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:163)\n\tat org.apache.xbean.asm9.ClassReader.<init>(ClassReader.java:284)\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:358)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2477)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$1(RDD.scala:860)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:859)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1231)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1222)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\n\tat breeze.optimize.OWLQN$$anon$1.calculate(OWLQN.scala:75)\n\tat breeze.optimize.OWLQN$$anon$1.calculate(OWLQN.scala:72)\n\tat breeze.optimize.BacktrackingLineSearch.iterations(BacktrackingLineSearch.scala:30)\n\tat breeze.optimize.ApproximateLineSearch.minimize(LineSearch.scala:26)\n\tat breeze.optimize.ApproximateLineSearch.minimize$(LineSearch.scala:26)\n\tat breeze.optimize.BacktrackingLineSearch.minimize(BacktrackingLineSearch.scala:11)\n\tat breeze.optimize.OWLQN.determineStepSize(OWLQN.scala:87)\n\tat breeze.optimize.OWLQN.determineStepSize(OWLQN.scala:17)\n\tat breeze.optimize.FirstOrderMinimizer.$anonfun$infiniteIterations$1(FirstOrderMinimizer.scala:60)\n\tat scala.collection.Iterator$$anon$7.next(Iterator.scala:140)\n\tat breeze.util.IteratorImplicits$RichIterator$$anon$2.next(Implicits.scala:74)\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1009)\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:628)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:495)\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:286)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.GeneratedMethodAccessor197.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 3\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 3\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 3\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 3\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
