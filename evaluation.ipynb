{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "# functions\n",
    "from functions import *\n",
    "import time\n",
    "# ML\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/06 01:06:37 WARN Utils: Your hostname, cuiyeshuaideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.31.122 instead (on interface en0)\n",
      "22/04/06 01:06:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/06 01:06:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"spark\").setMaster(\"local[*,20]\").set(\"spark.driver.memory\", \"10g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def giao():\n",
    "    file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "    # (file_name, ProvDocument)\n",
    "    document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "    # (file_name, Graphic_encoding_of_ProvDocument)\n",
    "    #encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "    encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "    # (file_name, prov_types of nodes)\n",
    "    if forward:\n",
    "        types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "        # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    else:\n",
    "        types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # (file_name, prov_types occurence in the graph)\n",
    "    types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "    # All prov_types in this collection of graphs\n",
    "    all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "    # Number of distinct prov_types\n",
    "    types_count = len(all_types)\n",
    "    print(types_count)\n",
    "    # index_map for prov_types, prov_type -> index\n",
    "    index_map = {all_types[i]: i for i in range(types_count)}\n",
    "    # index -> prov_type\n",
    "    reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "    # Contruct feature vectors for each graph\n",
    "    sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "    feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "    df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scaler_model = scaler.fit(df_features)\n",
    "    df_features = scaler_model.transform(df_features)\n",
    "    # Change the labels\n",
    "    df_labels = spark.read.csv(label_csv, header=True)\n",
    "    df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "    df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "    # Join the features and labels\n",
    "    df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "    # Split the data into training and testing\n",
    "    train, test = df.randomSplit([0.8, 0.2])\n",
    "    # Oversample the training data\n",
    "    labels = [float(x) for x in label_map.values()]\n",
    "    count = {}\n",
    "    for x in labels:\n",
    "        count[x] = train.filter(train['label'] == x).count()\n",
    "    maxValue = max(count.values())\n",
    "    ratio = {}\n",
    "    for x in labels:\n",
    "        ratio[x] = maxValue/count[x]\n",
    "    dataframes = []\n",
    "    for x in labels:\n",
    "        if(count[x] == maxValue):\n",
    "            dataframes.append(train.filter(train['label'] == x))\n",
    "        else:\n",
    "            dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x]))\n",
    "    train = dataframes[0]\n",
    "    for dataframe in dataframes[1:]:\n",
    "        train = train.union(dataframe)\n",
    "    print(ratio)\n",
    "    for x in labels:\n",
    "        print(train.filter(train['label'] == x).count())\n",
    "    \n",
    "    \n",
    "    #downsample the testing data\n",
    "    count = {}\n",
    "    for x in labels:\n",
    "        count[x] = test.filter(test['label'] == x).count()\n",
    "    minValue = min(count.values())\n",
    "    ratio = {}\n",
    "    for x in labels:\n",
    "        ratio[x] = minValue/count[x]\n",
    "    dataframes = []\n",
    "    for x in labels:\n",
    "        if(count[x] == minValue):\n",
    "            dataframes.append(test.filter(test['label'] == x))\n",
    "        else:\n",
    "            dataframes.append(test.filter(test['label'] == x).sample(withReplacement=False, fraction=ratio[x]))\n",
    "    test = dataframes[0]\n",
    "    for dataframe in dataframes[1:]:\n",
    "        test = test.union(dataframe)\n",
    "    print(ratio)\n",
    "    for x in labels:\n",
    "        print(test.filter(test['label'] == x).count())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # LinearSVC classifier\n",
    "    start = time.time()\n",
    "    svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "    pipeline = Pipeline(stages=[svc])\n",
    "    paramGrid = ParamGridBuilder().addGrid(svc.regParam, [0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 200]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "\n",
    "    # predict the labels of test data\n",
    "    res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "    # convert to dataframe and compute the metrics\n",
    "    preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    print(metrics.accuracy)\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "    paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "    # predict the labels of test data\n",
    "    res_test = cvModel.bestModel.transform(test)\n",
    "    # convert to dataframe and compute the metrics\n",
    "    preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    print(metrics.accuracy)\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[gbt])\n",
    "    paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline, \n",
    "        estimatorParamMaps=paramGrid, \n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "    # predict the labels of test data\n",
    "    res_test = cvModel.bestModel.transform(test)\n",
    "    # convert to dataframe and compute the metrics\n",
    "    preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    print(metrics.accuracy)\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    \n",
    "    start = time.time()\n",
    "    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "    pipeline = Pipeline(stages=[lr])\n",
    "    paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [5,10]).addGrid(lr.regParam, [0.2,0.3,0.4]).addGrid(lr.elasticNetParam, [0.6,0.7,0.8]).build()\n",
    "    # train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "    crossval = CrossValidator(\n",
    "        estimator=pipeline,\n",
    "        estimatorParamMaps=paramGrid,\n",
    "        evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"),\n",
    "        numFolds=10,\n",
    "        collectSubModels=True)\n",
    "    cvModel = crossval.fit(train)\n",
    "    # predict the labels of test data\n",
    "    res_test = cvModel.bestModel.transform(test)\n",
    "    # convert to dataframe and compute the metrics\n",
    "    preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "    metrics = MulticlassMetrics(preds_and_labels)\n",
    "    print(metrics.accuracy)\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 0\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 0\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 0\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 0\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/PG-T/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/PG-T/graphs.csv\"\n",
    "label_map = {\n",
    "    \"Valor\": \"0.0\",\n",
    "    \"Instinct\": \"1.0\",\n",
    "    \"Mystic\": \"2.0\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 1\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 2\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 3\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 3\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 3\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 3\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 4\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 5\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 6\n",
    "giao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 4\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 4\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 5\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 5\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "print(types_count)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 5\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 5\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 6\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 6\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 6\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 6\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = True\n",
    "specific_types_node = True\n",
    "level = 7\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = True\n",
    "specific_types_node = False\n",
    "level = 7\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = False\n",
    "specific_types_node = True\n",
    "level = 7\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/CM-Buildings/graphs.csv\"\n",
    "specific_types_edge = False\n",
    "specific_types_node = False\n",
    "level = 7\n",
    "iri = False\n",
    "forward = False\n",
    "qualified_names = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}\n",
    "label_map = {\n",
    "    \"Trusted\": \"1.0\",\n",
    "    \"Uncertain\": \"0.0\"\n",
    "}\n",
    "# label_map = {\n",
    "#     \"Valor\": 0.0,\n",
    "#     \"Instinct\": 1.0,\n",
    "#     \"Mystic\": 2.0\n",
    "# }\n",
    "# spark.sparkContext.addPyFile(\"functions.py\")\n",
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)\n",
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))\n",
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "#encoding_rdd = document_rdd.map(lambda x: (x[0], document_to_encoding(x[1],iri,forward)))\n",
    "encoding_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_encoding(x[1],iri,forward,qualified_names)))\n",
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate(x[1], level, specific_types_node, specific_types_edge)))\n",
    "    # types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_mixed(x[1], level, specific_types_node, specific_types_edge)))\n",
    "else:\n",
    "    types_rdd = encoding_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, specific_types_node, specific_types_edge)))\n",
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))\n",
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# Number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}\n",
    "# Contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)\n",
    "# Change the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.replace(label_map, subset=[\"label\"])\n",
    "df_labels = df_labels.withColumn(\"label\", df_labels[\"label\"].cast(DoubleType()))\n",
    "# Join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    "# Split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)\n",
    "# Oversample the data\n",
    "labels = [float(x) for x in label_map.values()]\n",
    "count = {}\n",
    "for x in labels:\n",
    "    count[x] = train.filter(train['label'] == x).count()\n",
    "maxValue = max(count.values())\n",
    "ratio = {}\n",
    "for x in labels:\n",
    "    ratio[x] = maxValue/count[x]\n",
    "dataframes = []\n",
    "for x in labels:\n",
    "    if(count[x] == maxValue):\n",
    "        dataframes.append(train.filter(train['label'] == x))\n",
    "    else:\n",
    "        dataframes.append(train.filter(train['label'] == x).sample(withReplacement=True, fraction=ratio[x], seed=123456))\n",
    "train = dataframes[0]\n",
    "for dataframe in dataframes[1:]:\n",
    "    train = train.union(dataframe)\n",
    "print(ratio)\n",
    "for x in labels:\n",
    "    print(train.filter(train['label'] == x).count())\n",
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01,0.001,0.0001,0.00001]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [5,10,20,30]).addGrid(rf.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "pipeline = Pipeline(stages=[gbt])\n",
    "paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [5,10]).addGrid(gbt.maxDepth, [3,4,5,6]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=10,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
