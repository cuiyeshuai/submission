{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, MapType, ArrayType, DoubleType\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import when\n",
    "# Prov\n",
    "import json\n",
    "from collections import Counter\n",
    "from prov.model import ProvDocument\n",
    "# ML\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LinearSVC, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'entity', 'activity': 'activity', 'wasGeneratedBy': 'wasGeneratedBy', 'used': 'used', 'wasInformedBy': 'wasInformedBy', 'wasStartedBy': 'wasStartedBy', 'wasEndedBy': 'wasEndedBy', 'wasInvalidatedBy': 'wasInvalidatedBy', 'wasDerivedFrom': 'wasDerivedFrom', 'agent': 'agent', 'wasAttributedTo': 'wasAttributedTo', 'wasAssociatedWith': 'wasAssociatedWith', 'actedOnBehalfOf': 'actedOnBehalfOf', 'wasInfluencedBy': 'wasInfluencedBy', 'alternateOf': 'alternateOf', 'specializationOf': 'specializationOf', 'hadMember': 'hadMember', 'bundle': 'bundle'}\n",
      "{'Entity': 'entity', 'Activity': 'activity', 'Generation': 'wasGeneratedBy', 'Usage': 'used', 'Communication': 'wasInformedBy', 'Start': 'wasStartedBy', 'End': 'wasEndedBy', 'Invalidation': 'wasInvalidatedBy', 'Derivation': 'wasDerivedFrom', 'Agent': 'agent', 'Attribution': 'wasAttributedTo', 'Association': 'wasAssociatedWith', 'Delegation': 'actedOnBehalfOf', 'Influence': 'wasInfluencedBy', 'Alternate': 'alternateOf', 'Specialization': 'specializationOf', 'Membership': 'hadMember', 'Bundle': 'bundle'}\n"
     ]
    }
   ],
   "source": [
    "PROV_ENTITY= \"entity\"\n",
    "PROV_ACTIVITY= \"activity\"\n",
    "PROV_GENERATION= \"wasGeneratedBy\"\n",
    "PROV_USAGE= \"used\"\n",
    "PROV_COMMUNICATION= \"wasInformedBy\"\n",
    "PROV_START= \"wasStartedBy\"\n",
    "PROV_END= \"wasEndedBy\"\n",
    "PROV_INVALIDATION= \"wasInvalidatedBy\"\n",
    "PROV_DERIVATION= \"wasDerivedFrom\"\n",
    "PROV_AGENT= \"agent\"\n",
    "PROV_ATTRIBUTION= \"wasAttributedTo\"\n",
    "PROV_ASSOCIATION= \"wasAssociatedWith\"\n",
    "PROV_DELEGATION= \"actedOnBehalfOf\"\n",
    "PROV_INFLUENCE= \"wasInfluencedBy\"\n",
    "PROV_ALTERNATE= \"alternateOf\"\n",
    "PROV_SPECIALIZATION= \"specializationOf\"\n",
    "PROV_MENTION= \"mentionOf\"\n",
    "PROV_MEMBERSHIP= \"hadMember\"\n",
    "PROV_BUNDLE= \"bundle\"\n",
    "PROV_MAP = {\n",
    "    PROV_ENTITY: \"entity\",\n",
    "    PROV_ACTIVITY: \"activity\",\n",
    "    PROV_GENERATION: \"wasGeneratedBy\",\n",
    "    PROV_USAGE: \"used\",\n",
    "    PROV_COMMUNICATION: \"wasInformedBy\",\n",
    "    PROV_START: \"wasStartedBy\",\n",
    "    PROV_END: \"wasEndedBy\",\n",
    "    PROV_INVALIDATION: \"wasInvalidatedBy\",\n",
    "    PROV_DERIVATION: \"wasDerivedFrom\",\n",
    "    PROV_AGENT: \"agent\",\n",
    "    PROV_ATTRIBUTION: \"wasAttributedTo\",\n",
    "    PROV_ASSOCIATION: \"wasAssociatedWith\",\n",
    "    PROV_DELEGATION: \"actedOnBehalfOf\",\n",
    "    PROV_INFLUENCE: \"wasInfluencedBy\",\n",
    "    PROV_ALTERNATE: \"alternateOf\",\n",
    "    PROV_SPECIALIZATION: \"specializationOf\",\n",
    "    PROV_MEMBERSHIP: \"hadMember\",\n",
    "    PROV_BUNDLE: \"bundle\",\n",
    "}\n",
    "PROV_N_MAP = {\n",
    "    \"Entity\": PROV_ENTITY,\n",
    "    \"Activity\": PROV_ACTIVITY,\n",
    "    \"Generation\": PROV_GENERATION,\n",
    "    \"Usage\": PROV_USAGE,\n",
    "    \"Communication\": PROV_COMMUNICATION,\n",
    "    \"Start\": PROV_START,\n",
    "    \"End\": PROV_END,\n",
    "    \"Invalidation\": PROV_INVALIDATION,\n",
    "    \"Derivation\": PROV_DERIVATION,\n",
    "    \"Agent\": PROV_AGENT,\n",
    "    \"Attribution\": PROV_ATTRIBUTION,\n",
    "    \"Association\": PROV_ASSOCIATION,\n",
    "    \"Delegation\": PROV_DELEGATION,\n",
    "    \"Influence\": PROV_INFLUENCE,\n",
    "    \"Alternate\": PROV_ALTERNATE,\n",
    "    \"Specialization\": PROV_SPECIALIZATION,\n",
    "    \"Membership\": PROV_MEMBERSHIP,\n",
    "    \"Bundle\": PROV_BUNDLE,\n",
    "}\n",
    "PROV_NODE = [\n",
    "    PROV_ENTITY,\n",
    "    PROV_ACTIVITY,\n",
    "    PROV_AGENT\n",
    "]\n",
    "PROV_EDGE = [\n",
    "    PROV_GENERATION,\n",
    "    PROV_USAGE,\n",
    "    PROV_COMMUNICATION,\n",
    "    PROV_START,\n",
    "    PROV_END,\n",
    "    PROV_INVALIDATION,\n",
    "    PROV_DERIVATION,\n",
    "    PROV_ATTRIBUTION,\n",
    "    PROV_ASSOCIATION,\n",
    "    PROV_DELEGATION,\n",
    "    PROV_INFLUENCE,\n",
    "    PROV_ALTERNATE,\n",
    "    PROV_SPECIALIZATION,\n",
    "    PROV_MEMBERSHIP\n",
    "]\n",
    "RELATION_MAP = {\n",
    "    PROV_GENERATION: (\"entity\", \"activity\"),\n",
    "    PROV_USAGE: (\"activity\", \"entity\"),\n",
    "    PROV_COMMUNICATION: (\"informed\", \"informant\"),\n",
    "    PROV_START: (\"activity\", \"entity\"),\n",
    "    PROV_END: (\"activity\", \"entity\"),\n",
    "    PROV_INVALIDATION: (\"entity\", \"activity\"),\n",
    "    PROV_DERIVATION: (\"generatedEntity\", \"usedEntity\"), # Revision\n",
    "    PROV_ATTRIBUTION: (\"entity\", \"agent\"),\n",
    "    PROV_ASSOCIATION: (\"activity\", \"agent\"), # agent plan\n",
    "    PROV_DELEGATION: (\"delegate\", \"responsible\"),\n",
    "    PROV_INFLUENCE: (\"influencee\", \"influencer\"),\n",
    "    PROV_ALTERNATE: (\"prov:alternate1\", \"prov:alternate2\"),\n",
    "    PROV_SPECIALIZATION: (\"specificEntity\", \"generalEntity\"),\n",
    "    PROV_MEMBERSHIP: (\"collection\", \"entity\")\n",
    "}\n",
    "PROV_RECORD_IDS_MAP = dict(\n",
    "    (PROV_MAP[rec_type_id], rec_type_id) for rec_type_id in PROV_MAP\n",
    ")\n",
    "PROV_RECORD_IDS_MAP_1 = dict(\n",
    "    (rec_type_id, PROV_N_MAP[rec_type_id]) for rec_type_id in PROV_N_MAP\n",
    ")\n",
    "# str -> constants\n",
    "print(PROV_RECORD_IDS_MAP)\n",
    "print(PROV_RECORD_IDS_MAP_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/PG-D/*.json\"\n",
    "label_csv = \"/Users/cuiyeshuai/Documents/UG modules/Individual Project/provenance-kernel-evaluation-master/datasets/PG-D/graphs.csv\"\n",
    "additional_types_edge = True\n",
    "additional_types_node = True\n",
    "level = 3\n",
    "uri = False\n",
    "forward = False\n",
    "qualified_name = {\n",
    "    \"xsd:QName\",\n",
    "    \"prov:QUALIFIED_NAME\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_records(x: ProvDocument, uri = uri) -> list:\n",
    "    output = ({}, {})  # (nodes, edges)\n",
    "    records = x.get_records()\n",
    "    for record in records:\n",
    "        if record.is_element():\n",
    "            # elements will be encoded into id -> (generic_type, specific_type)\n",
    "            res = (PROV_N_MAP[record.get_type().localpart],\n",
    "                   frozenset([assert_type.uri if uri else assert_type.__str__()\n",
    "                    for assert_type in record.get_asserted_types()])\n",
    "                   )\n",
    "            output[0][record.identifier.__str__()] = res\n",
    "        else:\n",
    "            # relations will be encoded into start_node -> [(end_node, relation_types)]\n",
    "            rec_type = PROV_N_MAP[record.get_type().localpart]\n",
    "            attributes = {}  # name -> value\n",
    "            for attribute in record.formal_attributes:\n",
    "                attributes[attribute[0].__str__()] = attribute[1].__str__()\n",
    "            relation = RELATION_MAP[rec_type]\n",
    "            # get starting and ending node of this relation(edge)\n",
    "            edge = attributes.get(\"prov:\" + relation[0]), attributes.get(\"prov:\" + relation[1])\n",
    "            types_in_str = {assert_type.__str__() for assert_type in record.get_asserted_types()}\n",
    "            if None in edge:\n",
    "                if rec_type == PROV_DERIVATION:\n",
    "                    if \"prov:Revision\" in types_in_str:\n",
    "                        edge = attributes.get(\"prov:generatedEntity\"), attributes.get(\"prov:usedEntity\")\n",
    "                elif rec_type == PROV_ASSOCIATION:\n",
    "                    edge = (attributes.get(\"prov:activity\"), attributes.get(\"prov:plan\"))\n",
    "            if None in edge:\n",
    "                continue\n",
    "            # get all relation types of this relation\n",
    "            res = (rec_type, frozenset([assert_type.uri if uri else assert_type.__str__()\n",
    "                              for assert_type in record.get_asserted_types()])) \n",
    "            if forward:\n",
    "                if output[1].get(edge[0]) == None:  # if start_node is not already in relations\n",
    "                    output[1][edge[0]] = [(edge[1], res)]\n",
    "                else:\n",
    "                    output[1][edge[0]].append((edge[1], res))\n",
    "            else:\n",
    "                if output[1].get(edge[1]) == None:  # if end_node is not already in relations\n",
    "                    output[1][edge[1]] = [(edge[0], res)]\n",
    "                else:\n",
    "                    output[1][edge[1]].append((edge[0], res))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_records(x: str) -> list:\n",
    "    output = ({}, {})  # (elements, relations)\n",
    "    file = json.decoder.JSONDecoder().decode(x)\n",
    "    if uri:\n",
    "        prefix = file[\"prefix\"]\n",
    "        prefix[\"prov\"] = \"http://www.w3.org/ns/prov#\"\n",
    "    del file[\"prefix\"]\n",
    "    if \"bundle\" in file:\n",
    "        del file[\"bundle\"]\n",
    "    for rec_type_str in file:\n",
    "        # file is dict\n",
    "        # rec_type_str is str\n",
    "        rec_type = PROV_RECORD_IDS_MAP[rec_type_str]\n",
    "        for rec_id, content in file[rec_type_str].items():\n",
    "            # rec_id in rec_type\n",
    "            if hasattr(content, \"items\"):  # it is a dict\n",
    "                #  There is only one element, create a singleton list\n",
    "                elements = [content]\n",
    "            else:\n",
    "                # expect it to be a list of dictionaries\n",
    "                elements = content\n",
    "\n",
    "            for element in elements:\n",
    "                res, types = None, frozenset()\n",
    "                if rec_type in PROV_NODE:\n",
    "                    if \"prov:type\" in element:\n",
    "                        if isinstance(element.get(\"prov:type\"), dict):\n",
    "                            element[\"prov:type\"] = [element[\"prov:type\"]] # make it a list\n",
    "                        types = frozenset([assert_type.get(\"$\") if assert_type.get(\"type\") in qualified_name else None\n",
    "                                 for assert_type in element[\"prov:type\"]])\n",
    "                        if uri:\n",
    "                            types = frozenset([prefix[type.split(\":\", 1)[0]] + type.split(\":\", 1)[1]\n",
    "                                     if type.split(\":\", 1)[0] in prefix else type for type in types])\n",
    "                    output[0][rec_id] = (rec_type, types)\n",
    "                else:\n",
    "                    relation = RELATION_MAP[rec_type]\n",
    "                    edge = element.get(\"prov:\" + relation[0]), element.get(\"prov:\" + relation[1])\n",
    "                    if \"prov:type\" in element:\n",
    "                        if isinstance(element.get(\"prov:type\"), dict):\n",
    "                            element[\"prov:type\"] = [element[\"prov:type\"]]\n",
    "                        types = frozenset([assert_type.get(\"$\") if assert_type.get(\"type\") in qualified_name else None\n",
    "                                 for assert_type in element[\"prov:type\"]])\n",
    "                    if None in edge:\n",
    "                        if rec_type == PROV_DERIVATION:\n",
    "                            if \"prov:Revision\" in types:\n",
    "                                edge = element.get(\"prov:generatedEntity\"), element.get(\"prov:usedEntity\")\n",
    "                        elif rec_type == PROV_ASSOCIATION:\n",
    "                            edge = (element.get(\"prov:activity\"), element.get(\"prov:plan\"))\n",
    "                    if None in edge:\n",
    "                        continue\n",
    "                    if uri:\n",
    "                        types = frozenset([prefix[type.split(\":\", 1)[0]] + type.split(\":\", 1)[1]\n",
    "                                 if type.split(\":\", 1)[0] in prefix else type for type in types])\n",
    "                    res = (rec_type, types)\n",
    "                    if forward:\n",
    "                        if output[1].get(edge[0]) == None:  # if start_node is not already in relations\n",
    "                            output[1][edge[0]] = [(edge[1], res)]\n",
    "                        else:\n",
    "                            output[1][edge[0]].append((edge[1], res))\n",
    "                    else:\n",
    "                        if output[1].get(edge[1]) == None:  # if end_node is not already in relations\n",
    "                            output[1][edge[1]] = [(edge[0], res)]\n",
    "                        else:\n",
    "                            output[1][edge[1]].append((edge[0], res))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_generate_mixed(x, level, additional_types_node, additional_types_edge):\n",
    "    zero_types = {}\n",
    "    for node in x[0]:\n",
    "        zero_types[node] = frozenset(x[0][node]) if additional_types_node else frozenset((x[0][node][0],))\n",
    "    h_types = {} # prov_types up to level h\n",
    "    for i in range(level + 1):\n",
    "        h_types[i] = {}\n",
    "    h_types[0] = {node: (zero_types[node],) for node in zero_types}\n",
    "    for i in range(1, level+1):\n",
    "        for source in x[1]: #iterate through all edges\n",
    "            for destination, edge_type in x[1][source]:\n",
    "                if destination in h_types[i-1]: #if the destination is in the previous level\n",
    "                    if h_types[i].get(source) is None:\n",
    "                        h_types[i][source] = ((frozenset(edge_type),) \n",
    "                            if additional_types_edge else (frozenset((edge_type[0],)),)) + h_types[i-1][destination] \n",
    "                    else: \n",
    "                        h_types[i][source] = tuple(m|n for m, n \n",
    "                            in zip(h_types[i][source], ((frozenset(edge_type),) \n",
    "                            if additional_types_edge else (frozenset((edge_type[0],)),)) + h_types[i-1][destination])) \n",
    "    return h_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_generate(x, level, additional_types_node, additional_types_edge):\n",
    "    zero_types = {}\n",
    "    for node in x[0]:\n",
    "        zero_types[node] = frozenset((x[0][node],)) if additional_types_node else frozenset((x[0][node][0],))\n",
    "    h_types = {} # prov_types up to level h\n",
    "    for i in range(level + 1):\n",
    "        h_types[i] = {}\n",
    "    h_types[0] = {node: (zero_types[node],) for node in zero_types}\n",
    "    for i in range(1, level+1):\n",
    "        for source in x[1]: #iterate through all edges\n",
    "            for destination, edge_type in x[1][source]:\n",
    "                if destination in h_types[i-1]: #if the destination is in the previous level\n",
    "                    if h_types[i].get(source) is None:\n",
    "                        h_types[i][source] = ((frozenset((edge_type,)),) if additional_types_edge else (frozenset((edge_type[0],)),)) + h_types[i-1][destination] \n",
    "                    else: \n",
    "                        h_types[i][source] = tuple(m|n for m, n in zip(h_types[i][source], ((frozenset((edge_type,)),) if additional_types_edge else (frozenset((edge_type[0],)),)) + h_types[i-1][destination])) \n",
    "    return h_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_generate_R(x, level, additional_types_node, additional_types_edge):\n",
    "    zero_types = {}\n",
    "    for node in x[0]:\n",
    "        zero_types[node] = frozenset((x[0][node],)) if additional_types_node else frozenset((x[0][node][0],))\n",
    "    h_types = {} # prov_types up to level h\n",
    "    for i in range(level + 1):\n",
    "        h_types[i] = {}\n",
    "    h_types[0] = {node: (zero_types[node],) for node in zero_types}\n",
    "    for i in range(1, level+1):\n",
    "        for destination in h_types[i-1]: # All nodes with h_types of level i-1\n",
    "            if destination in x[1]: # if the node is the destination of any edge\n",
    "                for source, edge_type in x[1][destination]:\n",
    "                    if h_types[i].get(source) is None:\n",
    "                        h_types[i][source] = ((frozenset((edge_type,)),) if additional_types_edge else (frozenset((edge_type[0],)),)) + h_types[i-1][destination] \n",
    "                    else: \n",
    "                        h_types[i][source] = tuple(m|n for m, n in zip(h_types[i][source], ((frozenset((edge_type,)),) if additional_types_edge else (frozenset((edge_type[0],)),)) + h_types[i-1][destination])) \n",
    "    return h_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_prov_types(level, h_types):\n",
    "    res = dict()  # Dict[prov_type, occurence]\n",
    "    for h in range(level + 1):\n",
    "        res.update(dict(Counter(h_types[h].values())))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_matrix(x, len_types, index_map):\n",
    "    res = [0] * len_types\n",
    "    for key in x:\n",
    "        res[index_map[key]] = x[key]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if Reverse is true then output will be features with most positive contribution\n",
    "def most_important_features(x, reverse_index_map, reverse=True):\n",
    "    feature_weight = [(x[i],i) for i in range(len(x))]\n",
    "    feature_weight.sort(reverse=reverse)\n",
    "    return [(reverse_index_map[i[1]], i[0]) for i in feature_weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"spark\").setMaster(\"local[8]\").set(\"spark.driver.memory\", \"2g\").set(\"spark.executor.memory\", \"2g\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into rdd (file_path, json_data(string))\n",
    "file_and_path_rdd = spark.sparkContext.wholeTextFiles(json_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (file_name, ProvDocument)\n",
    "document_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], (ProvDocument.deserialize(content=x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (file_name, Graphic_encoding_of_ProvDocument)\n",
    "# records_rdd_json = file_and_path_rdd.map(lambda x: (x[0], json_to_records(x[1])))\n",
    "records_rdd = document_rdd.map(lambda x: (x[0], document_to_records(x[1])))\n",
    "records_rdd = file_and_path_rdd.map(lambda x: (x[0].split(\"/\")[-1], json_to_records(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (file_name, prov_types of nodes)\n",
    "if forward:\n",
    "    types_rdd = records_rdd.map(lambda x: (x[0], type_generate(x[1], level, additional_types_node, additional_types_edge)))\n",
    "else:\n",
    "    types_rdd = records_rdd.map(lambda x: (x[0], type_generate_R(x[1], level, additional_types_node, additional_types_edge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (file_name, prov_types occurence in the graph)\n",
    "types_count_rdd = types_rdd.map(lambda x: (x[0], count_prov_types(level,x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in types_count_rdd.take(1):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All prov_types in this collection of graphs\n",
    "all_types = types_count_rdd.flatMap(lambda x: x[1].keys()).distinct().collect()\n",
    "# number of distinct prov_types\n",
    "types_count = len(all_types)\n",
    "# index_map for prov_types, prov_type -> index\n",
    "index_map = {all_types[i]: i for i in range(types_count)}\n",
    "# index -> prov_type\n",
    "reverse_index_map = {i: all_types[i] for i in range(types_count)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contruct feature vectors for each graph\n",
    "sparse_matrix_rdd = types_count_rdd.map(lambda x: (x[0], sparse_matrix(x[1], types_count, index_map)))\n",
    "feature_vector_rdd = sparse_matrix_rdd.map(lambda x: (x[0],Vectors.dense(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix_rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe from rdd\n",
    "df_features = spark.createDataFrame(feature_vector_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(df_features)\n",
    "df_features = scaler_model.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.select(\"scaledFeatures\").show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the labels\n",
    "df_labels = spark.read.csv(label_csv, header=True)\n",
    "df_labels = df_labels.withColumn(\"label\", when(df_labels.label == \"Trusted\", 1.0).otherwise(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the features and labels\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.scaledFeatures, df_labels.label).withColumnRenamed(\"scaledFeatures\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "train, test = df.randomSplit([0.8, 0.2], seed = 123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=BinaryClassificationEvaluator(), \n",
    "    numFolds=4,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearSVC classifier\n",
    "svc = LinearSVC(maxIter = 100, threshold=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "paramGrid = ParamGridBuilder().addGrid(svc.regParam, [1, 0.1, 0.01]).addGrid(svc.maxIter, [100, 500]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=4,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC()\n",
    "svc_model = svc.fit(train)\n",
    "res_test = svc_model.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_important_features(x, reverse_index_map, reverse=True):\n",
    "    feature_weight = [(x[i],i) for i in range(len(x))]\n",
    "    feature_weight.sort(reverse=reverse)\n",
    "    return [(reverse_index_map[i[1]], i[0]) for i in feature_weight]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_important_features(svc_model.coefficients.toArray(), reverse_index_map, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10,20,30]).build()\n",
    "# train the model and select the best model using \"metricName\"(hyperparameter tuning)\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline, \n",
    "    estimatorParamMaps=paramGrid, \n",
    "    evaluator=MulticlassClassificationEvaluator(metricName=\"accuracy\"), \n",
    "    numFolds=4,\n",
    "    collectSubModels=True)\n",
    "cvModel = crossval.fit(train)\n",
    "\n",
    "# predict the labels of test data\n",
    "res_test = cvModel.bestModel.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print(list(zip(cvModel.avgMetrics, paramGrid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "gbt_model = gbt.fit(train)\n",
    "res_test = gbt_model.transform(test)\n",
    "\n",
    "# convert to dataframe and compute the metrics\n",
    "preds_and_labels = res_test.select(\"prediction\", \"label\").rdd.map(lambda x: (x[0], x[1]))\n",
    "metrics = MulticlassMetrics(preds_and_labels)\n",
    "print(metrics.accuracy)\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = spark.createDataFrame(sparse_matrix_rdd).withColumnRenamed(\"_1\", \"file\").withColumnRenamed(\"_2\", \"features\")\n",
    "df = df_features.join(df_labels, df_features.file == df_labels.graph_file).select(df_features.features, df_labels.label)\n",
    "df_list = df.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = train.collect()\n",
    "test_list = test.collect()\n",
    "X_train = [x[0] for x in train_list]\n",
    "y_train = [x[1] for x in train_list]\n",
    "X_test = [x[0] for x in test_list]\n",
    "y_test = [x[1] for x in test_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# X = np.array([x[0] for x in df_list])\n",
    "# y = np.array([x[1] for x in df_list])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "    SVC(kernel=\"linear\"))\n",
    "clf.fit(X_train, y_train)\n",
    "(clf.predict(X_test), y_test)\n",
    "print(confusion_matrix(clf.predict(X_test), y_test))\n",
    "print(accuracy_score(clf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# X = np.array([x[0] for x in df_list])\n",
    "# y = np.array([x[1] for x in df_list])\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "    LinearSVC())\n",
    "clf.fit(X_train, y_train)\n",
    "(clf.predict(X_test), y_test)\n",
    "print(confusion_matrix(clf.predict(X_test), y_test))\n",
    "print(accuracy_score(clf.predict(X_test), y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
